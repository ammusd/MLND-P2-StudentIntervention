{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?\n",
    "\n",
    "**Answer:** This is a classification problem. \n",
    "In regression, the output is continous; whereas in classification, it is discrete. In this project, the goal is to find whether a student \"Needs Intervention\" or \"Does NOT Need Intervention\", constituting a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n",
    "_To execute a code cell, click inside it and press **Shift+Enter**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Graduation rate of the class: 67.09%\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "n_students = len(student_data)\n",
    "n_features = len(student_data.columns)-1\n",
    "n_passed = len(student_data[student_data['passed']=='yes'])\n",
    "n_failed = len(student_data[student_data['passed']=='no'])\n",
    "grad_rate = (float(n_passed)/float(n_students))*100\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)\n",
    "print \"Number of features: {}\".format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = (num_all - num_train)\n",
    "perc = float(num_test) / float(num_all)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "#X_train = X_all.sample(n=num_train)\n",
    "#y_train = y_all.sample(n=num_train)\n",
    "#X_test = X_all.sample(n=num_test)\n",
    "#y_test = y_all.sample(n=num_test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = perc, random_state=0) \n",
    "X_train_200 = X_train.sample(n=200, random_state=40)\n",
    "y_train_200 = y_train.sample(n=200, random_state=40)\n",
    "X_train_100 = X_train.sample(n=100, random_state=42)\n",
    "y_train_100 = y_train.sample(n=100, random_state=42)\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F1 score. Repeat   this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F1 score on training set and F1 score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "- In this project, the goal is to find whether a student \"Needs Intervention\" or \"Does NOT Need Intervention\", constituting a binary classification problem. The common/general application of all the techniques used below are for classification problems.\n",
    " \n",
    " **Model 1- Naive Bayes:** This is a classification technique based on Bayes theorem of probability to predict the class of unknown data set. \n",
    " \n",
    " GENERAL APPLICATIONS\n",
    " \n",
    "  - It can be applied to wide range of classification problems, such as text classification, word disambiguation and confidence measures for speech recognition etc.\n",
    " \n",
    " STRENGTHS\n",
    " \n",
    "  - It is one of the fastest classification techniques. \n",
    "  - It often provides good results at low cost in terms of model complexity.\n",
    " \n",
    " WEAKNESSES\n",
    " \n",
    "  - One of the disadvantages of Naive Bayes is that it has strong feature independence assumption. But in reality, usually there's some relation between features, hence it's \"Naive\" assuming so.\n",
    " \n",
    " REASONS FOR CHOOSING\n",
    "\n",
    " - This technique has a advantage of being one of the fastest classification techniques and also it provides less complex models.\n",
    " - Given our small dataset of only a few hundred rows, it would be better to train with a high bias classifier.\n",
    " - The model also has the advantage of being interpretable to a non-technical audience like the school board.\n",
    "\n",
    " **Model 2- Support Vector Machine:** \n",
    " \n",
    " GENERAL APPLICATIONS\n",
    " \n",
    " - SVM is a powerful classification technique and is used in a variety of applications, e.g. text and hypertext categorization, classification of images, hand-written characters recognition and many more.\n",
    " \n",
    " STRENGTHS\n",
    " \n",
    " - Training is relatively easy with this (unlike neural networks it has no local optima). \n",
    " - It scales relatively well to high dimensional data. \n",
    " - Tradeoff between classifier complexity and error can be controlled explicitly.\n",
    " - Non-traditional data like strings and trees can be used as input to SVM, instead of feature vectors.\n",
    "\n",
    "  WEAKNESSES\n",
    " \n",
    " - From a practical point of view perhaps the most serious problem with SVMs is the high algorithmic complexity and extensive memory requirements of the required quadratic programming in large-scale tasks.\n",
    " - It needs a “good” kernel function.\n",
    " \n",
    " REASONS FOR CHOOSING\n",
    "\n",
    " - This is very powerful, well known and widely used classification technique.\n",
    " - It can learn complex models even with relatively decent dataset sizes.\n",
    "  \n",
    " **Model 3- Logistic Regression:** \n",
    " \n",
    " GENERAL APPLICATIONS\n",
    " \n",
    " - Logistic regression is used widely in many settings, including the medical and social sciences. For example - to predict whether a patient has a given disease like  diabetes, coronary heart disease etc. \n",
    " - It can be used in engineering to predict the probability of failure of a given process or a system.\n",
    " - It's also widely used in marketing applications such as prediction of a customer's propensity to purchase a product.\n",
    " \n",
    " STRENGTHS\n",
    " \n",
    " - It is a well known technique, is quite easy to implement and fairly straight forward to understand.\n",
    " \n",
    " WEAKNESSES\n",
    " \n",
    " - Requires pre-processing in case of large number of predictors. \n",
    " \n",
    " REASONS FOR CHOOSING\n",
    "\n",
    " - It is a well known technique and easy to implement.\n",
    " - It is reliable with the given small data sets of few hundreds.\n",
    " - It can output probabilities of the outcome, i.e. be a soft-classifier - thus making the interpretation very easy and natural to understand to the student board.\n",
    "  \n",
    " **Model 4- Decision Tree:** In Decision Tree technique the model maps observations about an item to conclusions about the item's target value.\n",
    " \n",
    " GENERAL APPLICATIONS\n",
    " \n",
    " - The decision tree method is a powerful statistical tool for classification, prediction, interpretation, and data manipulation.\n",
    " - It has several potential applications in medical research, in biomedical Engineering for identifying features to be used in implantable devices and in manufacturing and production for semiconductor manufacturing, for increasing productivity and for quality control etc.\n",
    " \n",
    " STRENGTHS\n",
    " \n",
    " - It is simple to understand and interpret.\n",
    " - It requires little data preparation. \n",
    " - It can handle both numerical and categorical data.\n",
    " - It is robust, performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    " - It performs well with large datasets.\n",
    " \n",
    " WEAKNESSES\n",
    " \n",
    " - Instability - even a small change in input data can at times, cause large changes in the tree.\n",
    " - Complexity - decision tree learners can create over-complex trees that do not generalise well from the training data.\n",
    " - Unwieldy - large trees are not intelligible, and pose presentation difficulties.\n",
    " - Costs - decision tree analysis is an expensive option.\n",
    " \n",
    "  REASONS FOR CHOOSING\n",
    "\n",
    " - It is simple to understand and interpret.\n",
    " - It has an advantage of being flexible with both numerical and categorical data. \n",
    " - It is robust. \n",
    " \n",
    " \n",
    "- Just by understanding the problem statement we can conclude that the required output is binary. This narrows the selection of modeling techniques. Based on the given student intervention data, which has both categorical & continuous predictors, and considering the performance of the modeling techniques in terms of CPU utilization, the four modeling techniques chosen(Naive Bayes, SVM, Logistic Regression, Decision Tree) produce the desired outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>Naive Bayes</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.002 | 0.001 | 0.80 | 0.75 \n",
    "| Training 200 | 0.002 | 0.000 | 0.84 | 0.78 \n",
    "| Training 100 | 0.002 | 0.001 | 0.55 | 0.46 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>SVM</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.011 | 0.008 | 0.86 | 0.75 \n",
    "| Training 200 | 0.006 | 0.003 | 0.88 | 0.76 \n",
    "| Training 100 | 0.002 | 0.002 | 0.89 | 0.78 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>Logistic Regression</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.12 | 0.040 | 0.83 | 0.79 \n",
    "| Training 200 | 0.004 | 0.000 | 0.88 | 0.78 \n",
    "| Training 100 | 0.003 | 0.000 | 0.90 | 0.78 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>Decision Trees</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.003 | 0.000 | 1.0 | 0.72 \n",
    "| Training 200 | 0.003 | 0.001 | 1.0 | 0.73 \n",
    "| Training 100 | 0.002 | 0.000 | 1.0 | 0.72 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GaussianNB...\n",
      "Done!\n",
      "Training time (secs): 0.002\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "#Model using Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "#print clf  # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GaussianNB...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 0.808823529412\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GaussianNB...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training GaussianNB...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using GaussianNB...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 0.846666666667\n",
      "Predicting labels using GaussianNB...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.788321167883\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print \"------------------------------------------\"\n",
    "    print \"Training set size: {}\".format(len(X_train))\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    print \"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train))\n",
    "    print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))\n",
    "\n",
    "# TODO: Run the helper function above for desired subsets of training data\n",
    "# Note: Keep the test set constant\n",
    "\n",
    "#X_train1=X_train.sample(n=.75*num_train)\n",
    "# Train and predict using training set size=200\n",
    "train_predict(clf, X_train_200, y_train_200, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training GaussianNB...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using GaussianNB...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 0.559139784946\n",
      "Predicting labels using GaussianNB...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.46511627907\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=100\n",
    "train_predict(clf, X_train_100, y_train_100, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Train and predict using two other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.011\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.008\n",
      "F1 score for training set: 0.869198312236\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.758620689655\n"
     ]
    }
   ],
   "source": [
    "# Model using Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "clf2 = SVC()\n",
    "# Fit model to training data\n",
    "train_classifier(clf2, X_train, y_train) \n",
    "#Predict on training set and compute F1 score\n",
    "train_f1_score = predict_labels(clf2, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)\n",
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf2, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.006\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for training set: 0.882352941176\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.766233766234\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=200\n",
    "train_predict(clf2,X_train_200,y_train_200,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 0.895104895105\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.785714285714\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=100\n",
    "train_predict(clf2,X_train_100,y_train_100,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.120\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.040\n",
      "F1 score for training set: 0.838137472284\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.791044776119\n"
     ]
    }
   ],
   "source": [
    "# Model using Logistic regression\n",
    "from sklearn import linear_model \n",
    "clf3 = linear_model.LogisticRegression()\n",
    "# Fit model to training data\n",
    "train_classifier(clf3, X_train, y_train) \n",
    "#Predict on training set and compute F1 score\n",
    "train_f1_score = predict_labels(clf3, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)\n",
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf3, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.004\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 0.883435582822\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.788321167883\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=200\n",
    "train_predict(clf3,X_train_200,y_train_200,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 0.906474820144\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.787401574803\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=100\n",
    "train_predict(clf3,X_train_100,y_train_100,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.72131147541\n"
     ]
    }
   ],
   "source": [
    "# Model using Decision tree\n",
    "from sklearn import tree\n",
    "clf4 = tree.DecisionTreeClassifier()\n",
    "# Fit model to training data\n",
    "train_classifier(clf4, X_train, y_train) \n",
    "#Predict on training set and compute F1 score\n",
    "train_f1_score = predict_labels(clf4, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)\n",
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf4, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.738461538462\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=200\n",
    "train_predict(clf4,X_train_200,y_train_200,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.725663716814\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using training set size=100\n",
    "train_predict(clf4,X_train_100,y_train_100,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- Based on the experiments, all four models (Gaussian Naive Bayes, SVM, Logistic Regression and Decision Trees) take less than a second to perform classification and prediction. So timing wise, all models perform reasonably well and thus eliminate the need to compare models based on the computing time. Hence the error metric F1 score becomes the deciding factor to choose the best performing model. \n",
    "\n",
    " When we look at the F1 scores on training and the test set for various models, the **Logistic Regression model is the best** performing model of all.\n",
    "\n",
    " F1 score is a performance matric explained by a weighted average of precision and recall (F1 = 2 * (precision * recall) / (precision + recall)). It varies between 0 and 1. A higher F1 score indicates a better model. If we look at the F1 scores of the Logistic Regression models trained on various dataset sizes, the F1 score on training decreases as dataset size increases; but F1 score increases on the test set for the same models. This is inline with the learning curves theory, wherein the training error keeps on increasing as dataset size increases, and the corresponding error on test set decreases. This behavior is not seen with any of the models other than the Logistic Regression, thus Logistic Regression is considered the best in the given context.\n",
    "\n",
    " The F1 score on training dataset (300) for Logistic Regression and SVM models is almost same, but when we fit **SVM** on different training dataset sizes, F1 score increases for smaller training sizes (conversely, F1 decreases on larger training sets) and so does the F1 on corresponding test dataset. This shows that the model is perfoming better with fewer data (as opposed to working better with more data), which is possibly not correct. Hence this model is not chosen.\n",
    " \n",
    " On the **Decision Tree**, the F1 score is 1 for all the training dataset sizes and it is less than or equal to 0.75 for the test set. This shows that the model is overfitting on training data (classifying all the labels correctly), but fails to generalise on test. So this model is not chosen.\n",
    " \n",
    " For **Gaussian Naive Bayes** the F1 score is not consistent throughout the experiment between training sets and the test dataset. The incosistency is more so highlighted with the training set size of 100. This suggests that this algorithm probably needs more data, which is one of the disdvantages of Naive Bayes, to classify and generalise correctly.\n",
    "   \n",
    "   \n",
    "- Based on the results from various techniques, I would propose the Logistic Regression model as the best model for student intervention system. Because if we look at the results, though the time taken by all four models is almost the same (less than a second), F1 scores are different - thus F1 becomes the deciding factor. The F1 scores yielded by Logistic Regression seem to be the most consistent and reliable. Other models such as Naive Bayes clearly do not work best with fewer data points. Looking at the pattern of decreased training and test F1 scores with increase in training data indicate that SVM is not performing better with more data. Finally, the Decision Tree F1 scores of training and test show that the model is inconsistent. \n",
    "\n",
    " To explain in simple terms on how the Logistic Regression model works, it looks at each feature of the student e.g. age, gender, address, traveltime, studytime etc and learns to assign a weight to that feature which assesses the feature's importance in passing or failing the exam. Then it plugs in values into the model's equation by multiplying weights with corresponding feature values and summing them up, after which the results are input to the \"Logistic\" or \"Logit\" function. Finally the logit yeilds the probability that the student passes the exam and based on some threshold probability value (0.5 default), it classifies into Passed or Failed categories.\n",
    " \n",
    "  Thus the Logistic Regression model seems to perfom the best for the given student dataset. In simple terms, the Logistic Regression categorises the students into two groups: one group with students that have passed exams and another that have failed. Essentially the Logistic Regression algorithm learns how all the features (independent variables) contribute to student's performance (pass or fail the exam). The model built using the Logistic Regression on historical student data would  predict the possibility of students' performance on future exams. So using this model we can predict how the student might perform in future exams and thus can decide whether or not a particular student needs intervention to reach the goal of 95% graduation rate.\n",
    "<h2> Logistic Regression </h2>\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"bottom\">Fig: Logistic Regression Example Graph (at threshold 0.5)</caption>\n",
    "<tr><td><img src=\"http://vassarstats.net/lr1.gif\" alt=\"lr\" align=\"middle\" style=\"width:404px;height:328px;\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GridSearchCV...\n",
      "Done!\n",
      "Training time (secs): 7.884\n",
      "Predicting labels using GridSearchCV...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "Tuned F1 score for training set: 0.801033591731\n",
      "Predicting labels using GridSearchCV...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "Tuned F1 score for test set: 0.758064516129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "params={'C' :[.005,.05,.5,1.,10.,], \n",
    "        'fit_intercept' : [False],\n",
    "        'class_weight': ['balanced'],\n",
    "        'random_state' : [45],\n",
    "        'penalty': ['l1','l2'],\n",
    "        'n_jobs': [-1] \n",
    "       }\n",
    "scoring_function = make_scorer(f1_score, pos_label=\"yes\", greater_is_better=True)\n",
    "Logistic_grid = GridSearchCV(clf3,param_grid=params, scoring=scoring_function, n_jobs=-1,cv=3) \n",
    "train_classifier(Logistic_grid, X_train, y_train)\n",
    "#Predict on training set and compute F1 score\n",
    "train_f1_score = predict_labels(Logistic_grid, X_train, y_train)\n",
    "print \"Tuned F1 score for training set: {}\".format(train_f1_score)\n",
    "# Predict on test data\n",
    "print \"Tuned F1 score for test set: {}\".format(predict_labels(Logistic_grid, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After fine tuning with gridsearchcv with a default of 3 folds, the final F1 score of the trained model is 0.8 and the test is 0.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
